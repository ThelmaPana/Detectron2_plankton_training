{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Detectron2 training on plankton dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import json\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "import detectron2.data.transforms as T\n",
    "\n",
    "import lib.training_functions as training_functions # Custom training functions\n",
    "import lib.my_visualizer as my_visualizer #â€¯Custom Detectron2 visualizer to increase font size\n",
    "\n",
    "random.seed(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose output directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all output directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory\n",
    "output_dir = 'output/output_*'\n",
    "output_dirs = glob.glob(output_dir)\n",
    "output_dirs.sort()\n",
    "output_dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, select the last one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = output_dirs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(output_dir, 'settings.pickle'),'rb') as set_file:\n",
    "    settings = pickle.load(set_file)\n",
    "settings    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = settings['dataset']\n",
    "# Register the dataset to detectron2\n",
    "for d in ['train', 'valid', 'test']:\n",
    "    DatasetCatalog.register('plankton_' + d, lambda d=d: training_functions.my_dataset_function(os.path.join(data_dir, d)))\n",
    "    MetadataCatalog.get('plankton_' + d).set(thing_classes=['plankton'])\n",
    "plankton_metadata = MetadataCatalog.get('plankton_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have a look at a few training frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dicts = training_functions.format_bbox(os.path.join(data_dir, 'train')) \n",
    "for d in random.sample(dataset_dicts, 3):\n",
    "    img = cv2.imread(d['file_name'])\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=plankton_metadata, scale=1)\n",
    "    out = visualizer.draw_dataset_dict(d)\n",
    "    plt.figure(figsize = (10,10))\n",
    "    plt.imshow(out.get_image()[:, :, ::-1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_metrics = []\n",
    "with open(os.path.join(output_dir, 'metrics.json'), 'r') as f:\n",
    "    for line in f:\n",
    "        training_metrics.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training and validation loss evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "plt.plot(\n",
    "    [m['iteration'] for m in training_metrics if 'total_loss' in m], \n",
    "    [m['total_loss'] for m in training_metrics if 'total_loss' in m]\n",
    ")\n",
    "plt.plot(\n",
    "    [m['iteration'] for m in training_metrics if 'validation_loss' in m], \n",
    "    [m['validation_loss'] for m in training_metrics if 'validation_loss' in m])\n",
    "plt.legend(['total_loss', 'validation_loss'], loc='best')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "plt.plot(\n",
    "    [m['iteration'] for m in training_metrics if 'total_loss' in m], \n",
    "    [m['total_loss'] for m in training_metrics if 'total_loss' in m]\n",
    ")\n",
    "plt.plot(\n",
    "    [m['iteration'] for m in training_metrics if 'validation_loss' in m], \n",
    "    [m['validation_loss'] for m in training_metrics if 'validation_loss' in m])\n",
    "plt.legend(['total_loss', 'validation_loss'], loc='best')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylim([min(plt.ylim()),0.45])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(output_dir, 'test_results.pickle'),'rb') as results_file:\n",
    "    results = pickle.load(results_file)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision x recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=os.path.join(output_dir, 'precision_recall.png')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have a look at a few predicted frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model with default predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default config\n",
    "cfg = get_cfg()\n",
    "# Load training config\n",
    "cfg.merge_from_file(os.path.join(output_dir, 'my_cfg.yaml'))\n",
    "# Set detection threshold (can be played with)\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = settings['test_threshold']\n",
    "# Load model weights\n",
    "cfg.MODEL.WEIGHTS = os.path.join(output_dir, 'model_final.pth')\n",
    "\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_dicts = training_functions.format_bbox(os.path.join(data_dir, 'test'))\n",
    "sample_test = random.sample(test_dataset_dicts, 10)\n",
    "for d in sample_test:\n",
    "    im = cv2.imread(d['file_name'])\n",
    "    outputs = predictor(im)\n",
    "    v = my_visualizer.Visualizer(im[:, :, ::-1],\n",
    "                   metadata=plankton_metadata, \n",
    "                   scale=1\n",
    "    )\n",
    "    v = v.draw_instance_predictions(outputs['instances'].to('cpu'))\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(v.get_image()[:, :, ::-1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use single image predictor to predict 100 frames to see how it compares with batch prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_preds = {\n",
    "    'img_name': [],\n",
    "    'scores': [],\n",
    "}\n",
    "\n",
    "large_sample_test = random.sample(test_dataset_dicts, 100)\n",
    "for d in large_sample_test:\n",
    "    im = cv2.imread(d['file_name'])\n",
    "    outputs = predictor(im)\n",
    "    instances = outputs['instances']\n",
    "    if len(instances) > 0:\n",
    "\n",
    "        single_preds['img_name'].extend([d['file_name']] * len(instances))\n",
    "        single_preds['scores'].extend(instances.scores.cpu().numpy().tolist())\n",
    "single_preds = pd.DataFrame(single_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run batch inference on same frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create predictor for batch inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.modeling import build_model\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "\n",
    "# Get default config\n",
    "cfg = get_cfg()\n",
    "# Load training config\n",
    "cfg.merge_from_file(os.path.join(output_dir, 'my_cfg.yaml'))\n",
    "# Set detection threshold (can be played with)\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = settings['test_threshold']\n",
    "# Build model \n",
    "batch_predictor = build_model(cfg)\n",
    "# Load weights from training\n",
    "DetectionCheckpointer(batch_predictor).load(os.path.join(output_dir, 'model_final.pth'))\n",
    "# Change mode to eval\n",
    "batch_predictor.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare batch of frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate empty list for frames\n",
    "frames = []\n",
    "\n",
    "for d in sample_test:\n",
    "    # Read image\n",
    "    im = cv2.imread(d['file_name'])\n",
    "    height, width = im.shape[:2]\n",
    "    \n",
    "    # Resize image\n",
    "    aug1 = T.ResizeShortestEdge(short_edge_length=[800], max_size=1980, sample_style='choice')\n",
    "    frame = aug1.get_transform(im).apply_image(im)\n",
    "\n",
    "    # Reshape image from (H, W, C) to (C, H, W) for Detectron2 input and convert to tensol \n",
    "    frame = torch.as_tensor(frame.astype('uint8').transpose(2, 0, 1))\n",
    "\n",
    "    # Store in list of dicts with 'image', 'height' and 'width' entries\n",
    "    frames.append({'image': frame, 'height': height, 'width': width})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict batch of frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds = batch_predictor(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sample_test)):\n",
    "    # Get frame    \n",
    "    im = frames[i]['image'].numpy()\n",
    "    # Reshape it\n",
    "    im = np.moveaxis(im, 0, 2)\n",
    "    # Resize it\n",
    "    im = cv2.resize(im, preds[i]['instances'].to('cpu').image_size, interpolation = cv2.INTER_LINEAR) \n",
    "    \n",
    "    # Plot predictions\n",
    "    v = my_visualizer.Visualizer(im[:, :, ::-1],\n",
    "                   metadata=plankton_metadata, \n",
    "                   scale=1\n",
    "    )\n",
    "    v = v.draw_instance_predictions(preds[i]['instances'].to('cpu'))\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(v.get_image()[:, :, ::-1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate empty list for frames\n",
    "frames = []\n",
    "\n",
    "for d in large_sample_test:\n",
    "    # Read image\n",
    "    im = cv2.imread(d['file_name'])\n",
    "    height, width = im.shape[:2]\n",
    "    \n",
    "    # Resize image\n",
    "    aug1 = T.ResizeShortestEdge(short_edge_length=[800], max_size=1980, sample_style='choice')\n",
    "    frame = aug1.get_transform(im).apply_image(im)\n",
    "\n",
    "    # Reshape image from (H, W, C) to (C, H, W) for Detectron2 input and convert to tensol \n",
    "    frame = torch.as_tensor(frame.astype('uint8').transpose(2, 0, 1))\n",
    "\n",
    "    # Store in list of dicts with 'image', 'height' and 'width' entries\n",
    "    frames.append({'image': frame, 'height': height, 'width': width})\n",
    "    \n",
    "# Predict frames    \n",
    "with torch.no_grad():\n",
    "    preds = batch_predictor(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect batch prediction of frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_preds = {\n",
    "    'img_name': [],\n",
    "    'scores': [],\n",
    "}\n",
    "\n",
    "for d, pred in zip(large_sample_test, preds):\n",
    "    instances = pred['instances']\n",
    "    #break\n",
    "    if len(instances) > 0:\n",
    "        batch_preds['img_name'].extend([d['file_name']] * len(instances))\n",
    "        batch_preds['scores'].extend(instances.scores.cpu().numpy().tolist())\n",
    "batch_preds = pd.DataFrame(batch_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of predictions on the same 100 frames:')\n",
    "print(f'Single image predictor: {len(single_preds)}')\n",
    "print(f'Batch predictor: {len(batch_preds)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
